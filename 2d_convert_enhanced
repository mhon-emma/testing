import json
import os
import numpy as np
from pathlib import Path
import cv2
from typing import Dict, List, Tuple

class A2D2Converter:
    def __init__(self, a2d2_root_path: str):
        self.root_path = Path(a2d2_root_path)
        self.bbox_path = self.root_path / "camera_lidar_semantic_bboxes"
        
        # Load class list from the semantic dataset
        semantic_class_file = self.root_path / "camera_lidar_semantic" / "class_list.json"
        bbox_class_file = self.bbox_path / "class_list.json"
        
        # Use whichever class file exists
        if bbox_class_file.exists():
            with open(bbox_class_file, 'r') as f:
                self.class_list = json.load(f)
        elif semantic_class_file.exists():
            with open(semantic_class_file, 'r') as f:
                self.class_list = json.load(f)
        else:
            # Default classes based on A2D2 documentation
            self.class_list = {
                "Car": "Car",
                "VanSUV": "VanSUV", 
                "Truck": "Truck",
                "Pedestrian": "Pedestrian",
                "Bicycle": "Bicycle",
                "MotorBike": "MotorBike",
                "Bus": "Bus"
            }
        
        # Load camera configuration
        config_file = self.root_path / "cams_lidars.json"
        if config_file.exists():
            with open(config_file, 'r') as f:
                self.config = json.load(f)
        else:
            self.config = None
            print("Warning: cams_lidars.json not found. Will use image dimensions from files.")
    
    def project_3d_to_2d(self, points_3d: np.ndarray, camera_name: str) -> Tuple[np.ndarray, np.ndarray]:
        """Project 3D points to 2D using camera intrinsics"""
        if self.config is None:
            print("Warning: No camera config available")
            return np.array([]), np.array([])
            
        # Get camera intrinsic matrix (undistorted)
        cam_matrix = np.array(self.config['cameras'][camera_name]['CamMatrix'])
        
        # Filter out points behind the camera (z > 0 in camera coordinates)
        valid_mask = points_3d[:, 2] > 0
        valid_points = points_3d[valid_mask]
        
        if len(valid_points) == 0:
            return np.array([]), valid_mask
        
        # Project to 2D
        points_2d_homogeneous = cam_matrix @ valid_points.T
        points_2d = (points_2d_homogeneous[:2] / points_2d_homogeneous[2]).T
        
        return points_2d, valid_mask
    
    def extract_2d_bbox_from_3d_points(self, points_3d: List, camera_name: str, img_width: int, img_height: int) -> List:
        """Extract 2D bounding box from 3D corner points"""
        if not points_3d or len(points_3d) != 8:
            return []
            
        # Convert to numpy array
        corners_3d = np.array(points_3d)
        
        # Project to 2D
        corners_2d, valid_mask = self.project_3d_to_2d(corners_3d, camera_name)
        
        if len(corners_2d) == 0:
            return []
        
        # Get bounding box from projected corners
        x_coords = corners_2d[:, 0]
        y_coords = corners_2d[:, 1]
        
        x_min, x_max = np.min(x_coords), np.max(x_coords)
        y_min, y_max = np.min(y_coords), np.max(y_coords)
        
        # Clamp to image bounds
        x_min = max(0, min(x_min, img_width))
        x_max = max(0, min(x_max, img_width))
        y_min = max(0, min(y_min, img_height))
        y_max = max(0, min(y_max, img_height))
        
        # Only return valid bounding boxes
        if x_max > x_min and y_max > y_min:
            return [x_min, y_min, x_max - x_min, y_max - y_min]  # [x, y, width, height]
        
        return []
    
    def process_3d_labels(self, label3d_file: str, camera_name: str, img_width: int, img_height: int) -> List[Dict]:
        """Process 3D bounding box labels from A2D2 format"""
        try:
            with open(label3d_file, 'r') as f:
                label3d_data = json.load(f)
        except:
            return []
        
        annotations = []
        
        # Process each bounding box in the file
        for box_key, box_data in label3d_data.items():
            if not box_key.startswith('box_'):
                continue
                
            # Get class name
            class_name = box_data.get('class', 'Unknown')
            
            # Method 1: Use provided 2d_bbox if available and valid
            if '2d_bbox' in box_data and len(box_data['2d_bbox']) == 4:
                x_min, y_min, x_max, y_max = box_data['2d_bbox']
                bbox_2d = [x_min, y_min, x_max - x_min, y_max - y_min]
            
            # Method 2: Project 3D points to 2D if 3d_points available
            elif '3d_points' in box_data:
                bbox_2d = self.extract_2d_bbox_from_3d_points(
                    box_data['3d_points'], camera_name, img_width, img_height
                )
            
            # Method 3: Use center and size to estimate 3D corners, then project
            elif 'center' in box_data and 'size' in box_data:
                center = np.array(box_data['center'])
                size = np.array(box_data['size'])
                
                # Create 8 corners of 3D box
                half_size = size / 2
                corners_3d = np.array([
                    center + [-half_size[0], -half_size[1], -half_size[2]],
                    center + [+half_size[0], -half_size[1], -half_size[2]],
                    center + [+half_size[0], +half_size[1], -half_size[2]],
                    center + [-half_size[0], +half_size[1], -half_size[2]],
                    center + [-half_size[0], -half_size[1], +half_size[2]],
                    center + [+half_size[0], -half_size[1], +half_size[2]],
                    center + [+half_size[0], +half_size[1], +half_size[2]],
                    center + [-half_size[0], +half_size[1], +half_size[2]],
                ])
                
                # Apply rotation if available
                if 'axis' in box_data and 'rot_angle' in box_data:
                    axis = np.array(box_data['axis'])
                    angle = box_data['rot_angle']
                    
                    # Create rotation matrix from axis-angle
                    if np.linalg.norm(axis) > 0:
                        axis = axis / np.linalg.norm(axis)
                        cos_angle = np.cos(angle)
                        sin_angle = np.sin(angle)
                        
                        # Rodrigues' rotation formula
                        rotation_matrix = (cos_angle * np.eye(3) + 
                                         sin_angle * self._skew_symmetric(axis) +
                                         (1 - cos_angle) * np.outer(axis, axis))
                        
                        # Apply rotation to each corner
                        corners_3d = (rotation_matrix @ (corners_3d - center).T).T + center
                
                # Project to 2D
                corners_2d, valid_mask = self.project_3d_to_2d(corners_3d, camera_name)
                
                if len(corners_2d) > 0:
                    x_coords = corners_2d[:, 0]
                    y_coords = corners_2d[:, 1]
                    
                    x_min, x_max = np.min(x_coords), np.max(x_coords)
                    y_min, y_max = np.min(y_coords), np.max(y_coords)
                    
                    # Clamp to image bounds
                    x_min = max(0, min(x_min, img_width))
                    x_max = max(0, min(x_max, img_width))
                    y_min = max(0, min(y_min, img_height))
                    y_max = max(0, min(y_max, img_height))
                    
                    if x_max > x_min and y_max > y_min:
                        bbox_2d = [x_min, y_min, x_max - x_min, y_max - y_min]
                    else:
                        bbox_2d = []
                else:
                    bbox_2d = []
            else:
                bbox_2d = []
            
            # Add annotation if valid bounding box
            if bbox_2d and len(bbox_2d) == 4:
                annotations.append({
                    'class': class_name,
                    'bbox': bbox_2d,
                    'bbox_coords': [bbox_2d[0], bbox_2d[1], 
                                  bbox_2d[0] + bbox_2d[2], bbox_2d[1] + bbox_2d[3]],
                    'id': box_data.get('id', 0),
                    'truncation': box_data.get('truncation', 0.0),
                    'occlusion': box_data.get('occlusion', 0.0)
                })
        
        return annotations
    
    def _skew_symmetric(self, v):
        """Create skew symmetric matrix from vector"""
        return np.array([[0, -v[2], v[1]], 
                        [v[2], 0, -v[0]], 
                        [-v[1], v[0], 0]])
    
    def get_unique_classes(self) -> List[str]:
        """Get all unique class names from the dataset"""
        classes = set()
        
        # Process each sequence
        for sequence_dir in self.bbox_path.iterdir():
            if not sequence_dir.is_dir() or sequence_dir.name.startswith('.'):
                continue
            
            label3d_dir = sequence_dir / "label3D" / "cam_front_center"
            if not label3d_dir.exists():
                continue
            
            # Process each label file
            for label_file in label3d_dir.glob("*.json"):
                try:
                    with open(label_file, 'r') as f:
                        label_data = json.load(f)
                    
                    for box_key, box_data in label_data.items():
                        if box_key.startswith('box_') and 'class' in box_data:
                            classes.add(box_data['class'])
                except:
                    continue
        
        return sorted(list(classes))
    
    def convert_to_coco_format(self, output_file: str, resume: bool = True):
        """Convert A2D2 to COCO format with resume capability"""
        # Check if output file exists and resume is enabled
        if resume and Path(output_file).exists():
            print(f"Found existing file {output_file}, loading for resume...")
            try:
                with open(output_file, 'r') as f:
                    coco_data = json.load(f)
                print(f"Resuming from: {len(coco_data['images'])} images, {len(coco_data['annotations'])} annotations")
                
                # Get already processed files
                processed_files = {img['file_name'] for img in coco_data['images']}
                image_id = max([img['id'] for img in coco_data['images']], default=0)
                annotation_id = max([ann['id'] for ann in coco_data['annotations']], default=0)
            except:
                print("Error loading existing file, starting fresh...")
                coco_data = {"images": [], "annotations": [], "categories": []}
                processed_files = set()
                image_id = 0
                annotation_id = 0
        else:
            coco_data = {"images": [], "annotations": [], "categories": []}
            processed_files = set()
            image_id = 0
            annotation_id = 0
        
        # Get unique classes from the dataset
        unique_classes = self.get_unique_classes()
        print(f"Found classes: {unique_classes}")
        
        # Create categories (only if not resuming or categories empty)
        if not coco_data["categories"]:
            category_map = {}
            for idx, class_name in enumerate(unique_classes):
                category_id = idx + 1
                coco_data["categories"].append({
                    "id": category_id,
                    "name": class_name,
                    "supercategory": "object"
                })
                category_map[class_name] = category_id
        else:
            # Build category map from existing categories
            category_map = {cat['name']: cat['id'] for cat in coco_data["categories"]}
        
        # Process each sequence
        for sequence_dir in self.bbox_path.iterdir():
            if not sequence_dir.is_dir() or sequence_dir.name.startswith('.'):
                continue
            
            print(f"Processing sequence: {sequence_dir.name}")
            
            camera_dir = sequence_dir / "camera" / "cam_front_center"
            label3d_dir = sequence_dir / "label3D" / "cam_front_center"
            
            if not camera_dir.exists() or not label3d_dir.exists():
                continue
            
            # Process each image in the sequence
            for img_file in camera_dir.glob("*.png"):
                # Check if already processed
                file_name = f"{sequence_dir.name}_{img_file.name}"
                if file_name in processed_files:
                    continue
                    
                image_id += 1
                
                # Get corresponding JSON files
                base_name = img_file.stem
                camera_json = camera_dir / f"{base_name}.json"
                label3d_json = label3d_dir / f"{base_name}.json"
                
                if not label3d_json.exists():
                    continue
                
                # Load image to get dimensions
                try:
                    img = cv2.imread(str(img_file))
                    if img is None:
                        continue
                    height, width = img.shape[:2]
                except:
                    continue
                
                # Add image info
                coco_data["images"].append({
                    "id": image_id,
                    "file_name": file_name,
                    "width": width,
                    "height": height,
                    "sequence": sequence_dir.name
                })
                
                # Process 3D labels
                annotations = self.process_3d_labels(str(label3d_json), "front_center", width, height)
                
                for ann in annotations:
                    if ann['class'] in category_map:
                        annotation_id += 1
                        x, y, w, h = ann['bbox']
                        
                        coco_data["annotations"].append({
                            "id": annotation_id,
                            "image_id": image_id,
                            "category_id": category_map[ann['class']],
                            "bbox": [x, y, w, h],
                            "area": w * h,
                            "iscrowd": 0,
                            "truncation": ann.get('truncation', 0.0),
                            "occlusion": ann.get('occlusion', 0.0)
                        })
                
                # Save progress every 100 images
                if image_id % 100 == 0:
                    with open(output_file, 'w') as f:
                        json.dump(coco_data, f, indent=2)
                    print(f"Progress saved: {image_id} images processed")
        
        # Save COCO format
        with open(output_file, 'w') as f:
            json.dump(coco_data, f, indent=2)
        
        print(f"\nCOCO format saved to {output_file}")
        print(f"Total images: {len(coco_data['images'])}")
        print(f"Total annotations: {len(coco_data['annotations'])}")
        print(f"Categories: {[cat['name'] for cat in coco_data['categories']]}")
        
        # Generate dataset info for training frameworks
        self._generate_coco_dataset_info(output_file, coco_data)
    
    def convert_to_yolo_format(self, output_dir: str, resume: bool = True):
        """Convert A2D2 to YOLO format with resume capability"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Get unique classes from the dataset
        unique_classes = self.get_unique_classes()
        print(f"Found classes: {unique_classes}")
        
        # Create class names file
        classes_file = output_path / "classes.txt"
        with open(classes_file, 'w') as f:
            for class_name in unique_classes:
                f.write(f"{class_name}\n")
        
        # Create class mapping
        class_to_id = {name: idx for idx, name in enumerate(unique_classes)}
        
        # Get already processed files if resuming
        processed_files = set()
        if resume:
            existing_files = list(output_path.glob("*.txt"))
            processed_files = {f.stem for f in existing_files if f.name != "classes.txt"}
            if processed_files:
                print(f"Resuming: found {len(processed_files)} already processed files")
        
        image_count = 0
        annotation_count = 0
        
        # Process each sequence
        for sequence_dir in self.bbox_path.iterdir():
            if not sequence_dir.is_dir() or sequence_dir.name.startswith('.'):
                continue
            
            print(f"Processing sequence: {sequence_dir.name}")
            
            camera_dir = sequence_dir / "camera" / "cam_front_center"
            label3d_dir = sequence_dir / "label3D" / "cam_front_center"
            
            if not camera_dir.exists() or not label3d_dir.exists():
                continue
            
            # Process each image in the sequence
            for img_file in camera_dir.glob("*.png"):
                base_name = img_file.stem
                file_key = f"{sequence_dir.name}_{base_name}"
                
                # Skip if already processed
                if file_key in processed_files:
                    continue
                    
                label3d_json = label3d_dir / f"{base_name}.json"
                
                if not label3d_json.exists():
                    continue
                
                # Load image dimensions
                try:
                    img = cv2.imread(str(img_file))
                    if img is None:
                        continue
                    img_height, img_width = img.shape[:2]
                except:
                    continue
                
                # Process 3D labels
                annotations = self.process_3d_labels(str(label3d_json), "front_center", img_width, img_height)
                
                # Create YOLO annotation file
                yolo_file = output_path / f"{file_key}.txt"
                
                with open(yolo_file, 'w') as f:
                    for ann in annotations:
                        if ann['class'] in class_to_id:
                            class_id = class_to_id[ann['class']]
                            x_min, y_min, x_max, y_max = ann['bbox_coords']
                            
                            # Convert to YOLO format (normalized center coordinates)
                            center_x = (x_min + x_max) / 2 / img_width
                            center_y = (y_min + y_max) / 2 / img_height
                            width = (x_max - x_min) / img_width
                            height = (y_max - y_min) / img_height
                            
                            f.write(f"{class_id} {center_x:.6f} {center_y:.6f} {width:.6f} {height:.6f}\n")
                            annotation_count += 1
                
                image_count += 1
                
                # Print progress
                if image_count % 100 == 0:
                    print(f"Processed {image_count} images, {annotation_count} annotations")
        
        print(f"\nYOLO format saved to {output_dir}")
        print(f"Total images: {image_count}")
        print(f"Total annotations: {annotation_count}")
        print(f"Classes file: {output_path / 'classes.txt'}")
        print(f"Categories: {unique_classes}")
    
    def _generate_coco_dataset_info(self, output_file: str, coco_data: dict):
        """Generate additional files for COCO-based training frameworks"""
        output_path = Path(output_file).parent
        
        # Create dataset info file for frameworks like YOLOX, RT-DETR
        dataset_info = {
            "info": {
                "description": "A2D2 Dataset converted to COCO format",
                "version": "1.0",
                "year": 2024,
                "contributor": "A2D2 Converter",
                "date_created": "2024"
            },
            "licenses": [{
                "url": "https://creativecommons.org/licenses/by-nd/4.0/",
                "id": 1,
                "name": "CC BY ND 4.0"
            }],
            "dataset_stats": {
                "total_images": len(coco_data['images']),
                "total_annotations": len(coco_data['annotations']),
                "num_categories": len(coco_data['categories']),
                "categories": [cat['name'] for cat in coco_data['categories']]
            }
        }
        
        with open(output_path / "dataset_info.json", 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        # Create category mapping file (useful for some frameworks)
        category_mapping = {cat['name']: cat['id'] for cat in coco_data['categories']}
        with open(output_path / "category_mapping.json", 'w') as f:
            json.dump(category_mapping, f, indent=2)
        
        print(f"Generated additional COCO files:")
        print(f"  - {output_path / 'dataset_info.json'}")
        print(f"  - {output_path / 'category_mapping.json'}")
    
    def _generate_yolo_dataset_config(self, output_path: Path, classes: list, image_count: int, annotation_count: int):
        """Generate YOLO dataset configuration files"""
        
        # Generate data.yaml for YOLOv5/YOLOv8/YOLOv11/YOLOv12
        yolo_config = {
            'path': str(output_path.absolute()),
            'train': 'images/train',  # You'll need to organize images
            'val': 'images/val',
            'test': 'images/test',
            'nc': len(classes),
            'names': classes
        }
        
        with open(output_path / "data.yaml", 'w') as f:
            # Write YAML format
            f.write(f"# A2D2 Dataset Configuration\n")
            f.write(f"# Generated from A2D2 converter\n\n")
            f.write(f"path: {yolo_config['path']}\n")
            f.write(f"train: {yolo_config['train']}\n")
            f.write(f"val: {yolo_config['val']}\n")
            f.write(f"test: {yolo_config['test']}\n\n")
            f.write(f"# number of classes\n")
            f.write(f"nc: {yolo_config['nc']}\n\n")
            f.write(f"# class names\n")
            f.write(f"names:\n")
            for i, name in enumerate(classes):
                f.write(f"  {i}: {name}\n")
        
        # Generate dataset statistics
        stats = {
            "dataset_name": "A2D2",
            "total_images": image_count,
            "total_annotations": annotation_count,
            "num_classes": len(classes),
            "classes": classes,
            "format": "YOLO",
            "annotation_format": "class_id center_x center_y width height (normalized)",
            "coordinate_system": "normalized [0,1]"
        }
        
        with open(output_path / "dataset_stats.json", 'w') as f:
            json.dump(stats, f, indent=2)
        
        # Generate training recommendations
        recommendations = {
            "recommended_settings": {
                "YOLOv12": {
                    "model": "yolo12n/s/m/l/x.pt",
                    "imgsz": 640,
                    "batch": 16,
                    "epochs": 100,
                    "data": "data.yaml"
                },
                "YOLOv11": {
                    "model": "yolo11n/s/m/l/x.pt", 
                    "imgsz": 640,
                    "batch": 16,
                    "epochs": 100,
                    "data": "data.yaml"
                },
                "YOLOv8": {
                    "model": "yolov8n/s/m/l/x.pt",
                    "imgsz": 640,
                    "batch": 16,
                    "epochs": 100,
                    "data": "data.yaml"
                }
            },
            "notes": [
                "Adjust batch size based on your GPU memory",
                "Consider using larger image sizes (1280) for better accuracy",
                "Use data augmentation for better generalization",
                "Split data into train/val/test sets before training"
            ]
        }
        
        with open(output_path / "training_recommendations.json", 'w') as f:
            json.dump(recommendations, f, indent=2)
        
        print(f"Generated YOLO configuration files:")
        print(f"  - {output_path / 'data.yaml'}")
        print(f"  - {output_path / 'dataset_stats.json'}")
        print(f"  - {output_path / 'training_recommendations.json'}")
    
    def create_train_val_test_split(self, output_dir: str, train_ratio: float = 0.7, val_ratio: float = 0.2, test_ratio: float = 0.1):
        """Create train/validation/test splits for YOLO training"""
        import random
        from shutil import copy2
        
        output_path = Path(output_dir)
        if not output_path.exists():
            print("Run conversion first to generate YOLO format")
            return
        
        # Create directory structure
        for split in ['train', 'val', 'test']:
            (output_path / 'images' / split).mkdir(parents=True, exist_ok=True)
            (output_path / 'labels' / split).mkdir(parents=True, exist_ok=True)
        
        # Get all annotation files
        txt_files = list(output_path.glob("*.txt"))
        txt_files = [f for f in txt_files if f.name not in ['classes.txt', 'dataset_stats.json']]
        
        # Shuffle and split
        random.shuffle(txt_files)
        n_total = len(txt_files)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        train_files = txt_files[:n_train]
        val_files = txt_files[n_train:n_train + n_val]
        test_files = txt_files[n_train + n_val:]
        
        splits = {
            'train': train_files,
            'val': val_files, 
            'test': test_files
        }
        
        for split_name, files in splits.items():
            print(f"Moving {len(files)} files to {split_name} split...")
            for txt_file in files:
                # Move annotation file
                copy2(txt_file, output_path / 'labels' / split_name / txt_file.name)
                
                # Find and move corresponding image (you'll need to adjust image path)
                # This assumes images are in the same directory or you have a way to find them
                print(f"Note: You'll need to manually move corresponding images for {txt_file.name}")
        
        print(f"Split complete: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test")
        
        # Generate YOLO dataset configuration files
        self._generate_yolo_dataset_config(output_path, unique_classes, image_count, annotation_count)

# Usage example
if __name__ == "__main__":
    # Initialize converter with your A2D2 dataset path
    converter = A2D2Converter("/path/to/your/a2d2/dataset")
    
    # Convert to COCO format (with resume capability)
    converter.convert_to_coco_format("a2d2_coco.json", resume=True)
    
    # Convert to YOLO format (with resume capability)  
    converter.convert_to_yolo_format("a2d2_yolo", resume=True)
    
    # Optional: Create train/val/test splits for YOLO training
    # converter.create_train_val_test_split("a2d2_yolo", train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)