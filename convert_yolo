#!/usr/bin/env python3
"""
Final A2D2 to YOLO Converter - Based on actual A2D2 dataset structure
Handles: 2D Detection, 3D Detection, Semantic Segmentation

Key A2D2 Facts:
- 38 semantic segmentation classes
- 14 3D object detection classes  
- RGB semantic masks (not indexed)
- 41,277 frames with semantic labels
- 12,497 frames with 3D bounding boxes
"""

import os
import json
import numpy as np
import cv2
from PIL import Image
from pathlib import Path
import shutil
from tqdm import tqdm
import yaml

class FinalA2D2YOLOConverter:
    def __init__(self):
        self.a2d2_root = Path("/home/Lambdaone/Emma/a2d2_full")
        self.output_root = Path("/home/Lambdaone/Emma/a2d2_yolo")
        
        # A2D2 has 38 semantic classes and 14 object detection classes
        self.load_class_definitions()
        self.load_camera_config()
        self.setup_output_directories()
    
    def load_class_definitions(self):
        """Load A2D2 class definitions with proper mapping"""
        
        # A2D2 Semantic Segmentation Classes (38 classes)
        # These are the actual A2D2 semantic classes with RGB values
        self.semantic_classes = {
            "Car 1": [255, 0, 0],           # Red
            "Car 2": [200, 0, 0],           # Dark Red  
            "Car 3": [150, 0, 0],           # Darker Red
            "Car 4": [128, 0, 0],           # Darkest Red
            "Bicycle 1": [182, 89, 6],      # Orange-Brown
            "Bicycle 2": [150, 50, 4],      # Dark Orange-Brown
            "Bicycle 3": [90, 30, 1],       # Very Dark Orange-Brown
            "Bicycle 4": [90, 30, 30],      # Brown
            "Pedestrian 1": [204, 153, 255], # Light Purple
            "Pedestrian 2": [189, 73, 155],  # Purple  
            "Pedestrian 3": [239, 89, 191],  # Pink
            "Truck 1": [255, 128, 0],       # Orange
            "Truck 2": [200, 128, 0],       # Dark Orange
            "Truck 3": [150, 128, 0],       # Darker Orange
            "Small vehicles 1": [0, 255, 0], # Green
            "Small vehicles 2": [0, 200, 0], # Dark Green
            "Small vehicles 3": [0, 150, 0], # Darker Green
            "Traffic signal 1": [0, 128, 255], # Light Blue
            "Traffic signal 2": [30, 28, 158], # Dark Blue
            "Traffic signal 3": [60, 28, 100], # Purple-Blue
            "Traffic sign 1": [0, 255, 255],   # Cyan
            "Traffic sign 2": [30, 220, 220],  # Dark Cyan
            "Traffic sign 3": [60, 157, 199],  # Blue-Cyan
            "Utility vehicle 1": [255, 255, 0], # Yellow
            "Utility vehicle 2": [255, 255, 200], # Light Yellow
            "Sidebars": [233, 100, 0],        # Orange
            "Speed bumper": [110, 110, 0],    # Dark Yellow
            "Curbstone": [128, 128, 128],     # Gray
            "Solid line": [255, 255, 255],    # White
            "Irrelevant signs": [64, 170, 64], # Dark Green
            "Road blocks": [128, 64, 128],     # Purple-Gray
            "Tractor": [255, 0, 255],         # Magenta
            "Non-drivable street": [64, 64, 128], # Blue-Gray
            "Zebra crossing": [64, 0, 128],    # Dark Purple
            "Obstacles / trash": [64, 64, 0],  # Dark Yellow
            "Poles": [128, 128, 0],           # Olive
            "RD restricted area": [255, 128, 128], # Light Red
            "Animals": [64, 128, 64]          # Dark Green
        }
        
        # Try to load from file if available, otherwise use defaults
        self.load_semantic_from_file()
        self.load_bbox_classes_from_file()
        
        print(f"[INFO] Loaded {len(self.semantic_classes)} semantic classes")
        print(f"[INFO] Loaded {len(self.bbox_classes)} object detection classes")
    
    def load_semantic_from_file(self):
        """Try to load semantic classes from class_list.json"""
        class_file = self.a2d2_root / "camera_lidar_semantic" / "class_list.json"
        if class_file.exists():
            try:
                with open(class_file, 'r') as f:
                    file_classes = json.load(f)
                
                if isinstance(file_classes, dict):
                    # Override with file content if available
                    self.semantic_classes = file_classes
                    print(f"[OK] Loaded semantic classes from file: {class_file}")
                    
            except Exception as e:
                print(f"[WARNING] Could not load semantic classes from file: {e}")
                print("[INFO] Using default semantic classes")
    
    def load_bbox_classes_from_file(self):
        """Load 3D bounding box classes"""
        # A2D2 Object Detection Classes (14 classes typically)
        self.bbox_classes = [
            "Car", "Pedestrian", "Bicycle", "Truck", "Small vehicles",
            "Traffic signal", "Traffic sign", "Utility vehicle", 
            "Sidebars", "Speed bumper", "Curbstone", "Solid line",
            "Road blocks", "Tractor"
        ]
        
        class_file = self.a2d2_root / "camera_lidar_semantic_bboxes" / "class_list.json"
        if class_file.exists():
            try:
                with open(class_file, 'r') as f:
                    file_classes = json.load(f)
                
                if isinstance(file_classes, list):
                    self.bbox_classes = file_classes
                elif isinstance(file_classes, dict):
                    self.bbox_classes = list(file_classes.keys())
                    
                print(f"[OK] Loaded bbox classes from file: {class_file}")
                    
            except Exception as e:
                print(f"[WARNING] Could not load bbox classes from file: {e}")
                print("[INFO] Using default bbox classes")
    
    def load_camera_config(self):
        """Load camera configuration with proper error handling"""
        config_file = self.a2d2_root / "cams_lidars.json"
        self.camera_matrix = None
        
        if config_file.exists():
            try:
                with open(config_file, 'r') as f:
                    config = json.load(f)
                
                print(f"[DEBUG] Config file type: {type(config)}")
                print(f"[DEBUG] Config keys: {list(config.keys()) if isinstance(config, dict) else 'Not a dict'}")
                
                # Handle different possible structures
                cameras = None
                
                if isinstance(config, dict):
                    # Try different possible key names
                    if 'cameras' in config:
                        cameras = config['cameras']
                    elif 'camera' in config:
                        cameras = config['camera']  
                    elif 'sensors' in config:
                        sensors = config['sensors']
                        if isinstance(sensors, dict):
                            cameras = sensors.get('cameras', sensors.get('camera'))
                        elif isinstance(sensors, list):
                            cameras = [s for s in sensors if s.get('type') == 'camera']
                
                print(f"[DEBUG] Cameras found: {cameras is not None}")
                
                if cameras is not None:
                    # Handle cameras as list or dict
                    if isinstance(cameras, list):
                        for cam in cameras:
                            if isinstance(cam, dict) and cam.get('name') == 'cam_front_center':
                                self.extract_camera_matrix(cam)
                                break
                    elif isinstance(cameras, dict):
                        # Cameras stored as dictionary
                        front_cam = cameras.get('cam_front_center')
                        if front_cam:
                            self.extract_camera_matrix(front_cam)
                
                if self.camera_matrix is not None:
                    print(f"[OK] Loaded camera matrix successfully")
                else:
                    print(f"[WARNING] Could not find front center camera in config")
                    
            except json.JSONDecodeError as e:
                print(f"[ERROR] Invalid JSON in config file: {e}")
            except Exception as e:
                print(f"[ERROR] Error loading camera config: {e}")
                print(f"[DEBUG] Config content preview: {str(config)[:200]}...")
        else:
            print(f"[WARNING] Config file not found: {config_file}")
        
        # Set default camera matrix if not found
        if self.camera_matrix is None:
            self.camera_matrix = np.array([
                [1000, 0, 960],    # fx=1000, cx=960 (assuming 1920px width)
                [0, 1000, 540],    # fy=1000, cy=540 (assuming 1080px height)
                [0, 0, 1]
            ])
            print("[INFO] Using default camera matrix")
    
    def extract_camera_matrix(self, cam_data):
        """Extract camera matrix from camera data"""
        try:
            if isinstance(cam_data, dict):
                # Try different possible key names for camera matrix
                matrix_keys = ['CameraMatrix', 'camera_matrix', 'intrinsic', 'K', 'calibration_matrix']
                matrix = None
                
                for key in matrix_keys:
                    if key in cam_data:
                        matrix = cam_data[key]
                        break
                
                if matrix is not None and len(matrix) >= 9:
                    if isinstance(matrix, list) and len(matrix) == 9:
                        # 3x3 matrix flattened
                        self.camera_matrix = np.array([
                            [matrix[0], matrix[1], matrix[2]],
                            [matrix[3], matrix[4], matrix[5]], 
                            [matrix[6], matrix[7], matrix[8]]
                        ])
                    elif isinstance(matrix, list) and len(matrix) == 3 and all(len(row) == 3 for row in matrix):
                        # Already 3x3 matrix
                        self.camera_matrix = np.array(matrix)
                    else:
                        print(f"[WARNING] Unexpected camera matrix format: {matrix}")
                        
                    if self.camera_matrix is not None:
                        fx = self.camera_matrix[0, 0]
                        fy = self.camera_matrix[1, 1]
                        cx = self.camera_matrix[0, 2]
                        cy = self.camera_matrix[1, 2]
                        print(f"[OK] Camera matrix: fx={fx:.1f}, fy={fy:.1f}, cx={cx:.1f}, cy={cy:.1f}")
                        
        except Exception as e:
            print(f"[ERROR] Error extracting camera matrix: {e}")
    
    def setup_output_directories(self):
        """Create output directory structure"""
        tasks = ['2d_detection', '3d_detection', 'segmentation']
        splits = ['train', 'val', 'test']
        
        for task in tasks:
            for split in splits:
                (self.output_root / task / split / 'images').mkdir(parents=True, exist_ok=True)
                (self.output_root / task / split / 'labels').mkdir(parents=True, exist_ok=True)
                
                if task == '3d_detection':
                    (self.output_root / task / split / 'point_clouds').mkdir(parents=True, exist_ok=True)
        
        print(f"[OK] Created output directories at: {self.output_root}")
    
    def rgb_to_class_id(self, rgb_mask):
        """Convert RGB semantic mask to class indices"""
        if rgb_mask is None or len(rgb_mask.shape) != 3:
            return np.zeros((100, 100), dtype=np.uint8)
        
        h, w = rgb_mask.shape[:2]
        class_mask = np.zeros((h, w), dtype=np.uint8)
        
        # Convert RGB values to class IDs
        for class_id, (class_name, rgb_values) in enumerate(self.semantic_classes.items()):
            if isinstance(rgb_values, list) and len(rgb_values) == 3:
                r, g, b = rgb_values
                # Create boolean mask for this color
                mask = (rgb_mask[:, :, 0] == r) & (rgb_mask[:, :, 1] == g) & (rgb_mask[:, :, 2] == b)
                class_mask[mask] = class_id
        
        return class_mask
    
    def project_3d_to_2d(self, point_3d, camera_matrix, img_width, img_height):
        """Project 3D point to 2D image coordinates"""
        if len(point_3d) < 3:
            return None
        
        # Check if point is behind camera
        if point_3d[2] <= 0:
            return None
        
        # Project to image plane
        point_2d_homo = camera_matrix @ np.array(point_3d)
        
        if point_2d_homo[2] == 0:
            return None
        
        x = point_2d_homo[0] / point_2d_homo[2]
        y = point_2d_homo[1] / point_2d_homo[2]
        
        # Check if point is within image bounds
        if 0 <= x < img_width and 0 <= y < img_height:
            return [x, y]
        
        return None
    
    def convert_3d_bbox_to_2d_yolo(self, bbox_3d, camera_matrix, img_width, img_height):
        """Convert 3D bounding box to 2D YOLO format"""
        try:
            center = bbox_3d.get('center', [0, 0, 0])
            size = bbox_3d.get('size', [1, 1, 1])
            rotation = bbox_3d.get('rotation', [0, 0, 0])
            
            # Generate 8 corners of 3D box
            l, w, h = size
            corners_3d = np.array([
                [-l/2, -w/2, -h/2], [l/2, -w/2, -h/2],
                [l/2, w/2, -h/2], [-l/2, w/2, -h/2],
                [-l/2, -w/2, h/2], [l/2, -w/2, h/2],
                [l/2, w/2, h/2], [-l/2, w/2, h/2]
            ])
            
            # Translate to center position
            corners_3d += center
            
            # Project all corners to 2D
            projected_corners = []
            for corner in corners_3d:
                point_2d = self.project_3d_to_2d(corner, camera_matrix, img_width, img_height)
                if point_2d:
                    projected_corners.append(point_2d)
            
            if len(projected_corners) < 4:
                return None
            
            # Get 2D bounding box
            projected_corners = np.array(projected_corners)
            x_min = max(0, projected_corners[:, 0].min())
            y_min = max(0, projected_corners[:, 1].min())
            x_max = min(img_width, projected_corners[:, 0].max())
            y_max = min(img_height, projected_corners[:, 1].max())
            
            # Convert to YOLO format
            x_center = (x_min + x_max) / 2 / img_width
            y_center = (y_min + y_max) / 2 / img_height
            width = (x_max - x_min) / img_width
            height = (y_max - y_min) / img_height
            
            # Validate bbox
            if width > 0 and height > 0 and width <= 1 and height <= 1:
                return [x_center, y_center, width, height]
            
        except Exception as e:
            print(f"[WARNING] Error converting 3D bbox: {e}")
        
        return None
    
    def mask_to_yolo_polygons(self, class_mask, class_id, img_width, img_height):
        """Convert binary mask to YOLO polygon format"""
        binary_mask = (class_mask == class_id).astype(np.uint8)
        
        # Find contours
        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        polygons = []
        for contour in contours:
            # Filter small contours
            if cv2.contourArea(contour) < 50:
                continue
            
            # Simplify contour
            epsilon = 0.02 * cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, epsilon, True)
            
            if len(approx) >= 3:
                # Normalize coordinates
                polygon = approx.reshape(-1, 2).astype(np.float32)
                polygon[:, 0] /= img_width
                polygon[:, 1] /= img_height
                
                # Clip to valid range
                polygon = np.clip(polygon, 0, 1)
                
                # Format for YOLO segmentation
                coords = polygon.flatten()
                if len(coords) >= 6:  # At least 3 points
                    polygons.append(coords)
        
        return polygons
    
    def split_sequences(self, sequences, train_ratio=0.7, val_ratio=0.15):
        """Split sequences into train/val/test"""
        n_total = len(sequences)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        # Sort sequences for consistent splitting
        sequences = sorted(sequences, key=lambda x: x.name)
        
        train_seqs = sequences[:n_train]
        val_seqs = sequences[n_train:n_train + n_val]
        test_seqs = sequences[n_train + n_val:]
        
        return train_seqs, val_seqs, test_seqs
    
    def process_2d_detection(self):
        """Process 2D object detection from 3D bboxes"""
        print("\nProcessing 2D Object Detection...")
        
        bbox_root = self.a2d2_root / "camera_lidar_semantic_bboxes"
        sequences = list(bbox_root.glob("2018*"))
        
        if not sequences:
            print("[ERROR] No sequences found for 3D bboxes")
            return
        
        train_seqs, val_seqs, test_seqs = self.split_sequences(sequences)
        
        splits = [
            (train_seqs, "train"),
            (val_seqs, "val"),
            (test_seqs, "test")
        ]
        
        total_processed = 0
        
        for seq_list, split_name in splits:
            if not seq_list:
                continue
            
            print(f"Processing {split_name} split ({len(seq_list)} sequences)...")
            
            for seq_dir in tqdm(seq_list, desc=f"{split_name}"):
                camera_dir = seq_dir / "camera" / "cam_front_center"
                label3d_dir = seq_dir / "label3D" / "cam_front_center"
                
                if not camera_dir.exists() or not label3d_dir.exists():
                    continue
                
                for img_file in camera_dir.glob("*.png"):
                    # Get corresponding 3D label file
                    label_file = label3d_dir / img_file.name.replace("camera", "label3D").replace(".png", ".json")
                    
                    if not label_file.exists():
                        continue
                    
                    try:
                        # Load image
                        img = cv2.imread(str(img_file))
                        if img is None:
                            continue
                        
                        h, w = img.shape[:2]
                        
                        # Load 3D annotations
                        with open(label_file, 'r') as f:
                            annotations_3d = json.load(f)
                        
                        yolo_annotations = []
                        
                        # Process each 3D bbox
                        for bbox_3d in annotations_3d:
                            class_name = bbox_3d.get('class', '')
                            
                            # Map class name to ID
                            if class_name in self.bbox_classes:
                                class_id = self.bbox_classes.index(class_name)
                            else:
                                continue
                            
                            # Convert 3D to 2D
                            bbox_2d = self.convert_3d_bbox_to_2d_yolo(bbox_3d, self.camera_matrix, w, h)
                            
                            if bbox_2d:
                                annotation = f"{class_id} {bbox_2d[0]:.6f} {bbox_2d[1]:.6f} {bbox_2d[2]:.6f} {bbox_2d[3]:.6f}"
                                yolo_annotations.append(annotation)
                        
                        # Save files
                        out_img = self.output_root / "2d_detection" / split_name / "images" / img_file.name
                        out_label = self.output_root / "2d_detection" / split_name / "labels" / img_file.with_suffix('.txt').name
                        
                        shutil.copy2(img_file, out_img)
                        
                        with open(out_label, 'w') as f:
                            f.write('\n'.join(yolo_annotations))
                        
                        total_processed += 1
                        
                    except Exception as e:
                        continue
        
        print(f"[OK] 2D detection: {total_processed} images processed")
    
    def process_semantic_segmentation(self):
        """Process semantic segmentation with proper polygon conversion"""
        print("\nProcessing Semantic Segmentation...")
        
        semantic_root = self.a2d2_root / "camera_lidar_semantic"
        sequences = list(semantic_root.glob("2018*"))
        
        if not sequences:
            print("[ERROR] No sequences found for semantic segmentation")
            return
        
        train_seqs, val_seqs, test_seqs = self.split_sequences(sequences)
        
        splits = [
            (train_seqs, "train"),
            (val_seqs, "val"),
            (test_seqs, "test")
        ]
        
        total_processed = 0
        
        for seq_list, split_name in splits:
            if not seq_list:
                continue
            
            print(f"Processing {split_name} split ({len(seq_list)} sequences)...")
            
            for seq_dir in tqdm(seq_list, desc=f"{split_name}"):
                camera_dir = seq_dir / "camera" / "cam_front_center"
                label_dir = seq_dir / "label" / "cam_front_center"
                
                if not camera_dir.exists() or not label_dir.exists():
                    continue
                
                for img_file in camera_dir.glob("*.png"):
                    # Get corresponding semantic label
                    label_file = label_dir / img_file.name.replace("camera", "label")
                    
                    if not label_file.exists():
                        continue
                    
                    try:
                        # Load image and mask
                        img = cv2.imread(str(img_file))
                        mask_img = cv2.imread(str(label_file))
                        
                        if img is None or mask_img is None:
                            continue
                        
                        h, w = img.shape[:2]
                        
                        # Convert BGR to RGB for proper color matching
                        mask_rgb = cv2.cvtColor(mask_img, cv2.COLOR_BGR2RGB)
                        
                        # Convert RGB mask to class indices
                        class_mask = self.rgb_to_class_id(mask_rgb)
                        
                        # Generate YOLO segmentation annotations
                        yolo_annotations = []
                        unique_classes = np.unique(class_mask)
                        
                        for class_id in unique_classes:
                            if class_id == 0:  # Skip background/unknown
                                continue
                            
                            # Convert mask to polygons
                            polygons = self.mask_to_yolo_polygons(class_mask, class_id, w, h)
                            
                            for polygon in polygons:
                                coords_str = ' '.join([f"{coord:.6f}" for coord in polygon])
                                annotation = f"{class_id} {coords_str}"
                                yolo_annotations.append(annotation)
                        
                        # Save files
                        out_img = self.output_root / "segmentation" / split_name / "images" / img_file.name
                        out_label = self.output_root / "segmentation" / split_name / "labels" / img_file.with_suffix('.txt').name
                        
                        shutil.copy2(img_file, out_img)
                        
                        with open(out_label, 'w') as f:
                            f.write('\n'.join(yolo_annotations))
                        
                        total_processed += 1
                        
                    except Exception as e:
                        continue
        
        print(f"[OK] Segmentation: {total_processed} images processed")
    
    def process_3d_detection(self):
        """Process 3D detection data (experimental - for future 3D models)"""
        print("\nProcessing 3D Detection...")
        print("[INFO] Note: Standard YOLOv12 doesn't support 3D detection")
        print("[INFO] This prepares data for specialized 3D models")
        
        bbox_root = self.a2d2_root / "camera_lidar_semantic_bboxes"
        sequences = list(bbox_root.glob("2018*"))
        
        if not sequences:
            print("[ERROR] No sequences found for 3D bboxes")
            return
        
        train_seqs, val_seqs, test_seqs = self.split_sequences(sequences)
        
        splits = [
            (train_seqs, "train"),
            (val_seqs, "val"),
            (test_seqs, "test")
        ]
        
        total_processed = 0
        
        for seq_list, split_name in splits:
            if not seq_list:
                continue
            
            print(f"Processing {split_name} split ({len(seq_list)} sequences)...")
            
            for seq_dir in tqdm(seq_list, desc=f"{split_name}"):
                camera_dir = seq_dir / "camera" / "cam_front_center"
                label3d_dir = seq_dir / "label3D" / "cam_front_center"
                lidar_dir = seq_dir / "lidar" / "cam_front_center"
                
                if not all([camera_dir.exists(), label3d_dir.exists(), lidar_dir.exists()]):
                    continue
                
                for img_file in camera_dir.glob("*.png"):
                    label_file = label3d_dir / img_file.name.replace("camera", "label3D").replace(".png", ".json")
                    lidar_file = lidar_dir / img_file.name.replace("camera", "lidar").replace(".png", ".npz")
                    
                    if not label_file.exists() or not lidar_file.exists():
                        continue
                    
                    try:
                        # Load 3D annotations
                        with open(label_file, 'r') as f:
                            annotations_3d = json.load(f)
                        
                        # Process 3D annotations
                        yolo_3d_annotations = []
                        
                        for bbox_3d in annotations_3d:
                            class_name = bbox_3d.get('class', '')
                            
                            if class_name in self.bbox_classes:
                                class_id = self.bbox_classes.index(class_name)
                            else:
                                continue
                            
                            center = bbox_3d.get('center', [0, 0, 0])
                            size = bbox_3d.get('size', [1, 1, 1])
                            rotation = bbox_3d.get('rotation', [0, 0, 0])
                            
                            # Format: class_id x y z l w h rotation_y
                            annotation = f"{class_id} {center[0]:.6f} {center[1]:.6f} {center[2]:.6f} {size[0]:.6f} {size[1]:.6f} {size[2]:.6f} {rotation[2]:.6f}"
                            yolo_3d_annotations.append(annotation)
                        
                        # Save files
                        out_img = self.output_root / "3d_detection" / split_name / "images" / img_file.name
                        out_label = self.output_root / "3d_detection" / split_name / "labels" / img_file.with_suffix('.txt').name
                        out_lidar = self.output_root / "3d_detection" / split_name / "point_clouds" / lidar_file.name
                        
                        shutil.copy2(img_file, out_img)
                        shutil.copy2(lidar_file, out_lidar)
                        
                        with open(out_label, 'w') as f:
                            f.write('\n'.join(yolo_3d_annotations))
                        
                        total_processed += 1
                        
                    except Exception as e:
                        continue
        
        print(f"[OK] 3D detection: {total_processed} images processed")
    
    def create_dataset_configs(self):
        """Create YOLO dataset configuration files"""
        print("\nCreating dataset configurations...")
        
        configs = {
            '2d_detection_config.yaml': {
                'path': str(self.output_root / '2d_detection'),
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'nc': len(self.bbox_classes),
                'names': self.bbox_classes
            },
            'segmentation_config.yaml': {
                'path': str(self.output_root / 'segmentation'),
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'nc': len(self.semantic_classes),
                'names': list(self.semantic_classes.keys())
            },
            '3d_detection_config.yaml': {
                'path': str(self.output_root / '3d_detection'),
                'train': 'train/images',
                'val': 'val/images',
                'test': 'test/images',
                'nc': len(self.bbox_classes),
                'names': self.bbox_classes,
                'point_clouds': True,
                'format': '3d'
            }
        }
        
        for config_name, config_data in configs.items():
            config_path = self.output_root / config_name
            with open(config_path, 'w') as f:
                yaml.dump(config_data, f, default_flow_style=False)
            print(f"[OK] Created: {config_path}")
    
    def run_conversion(self):
        """Run complete A2D2 to YOLO conversion"""
        print("=" * 60)
        print("A2D2 TO YOLO CONVERTER - FINAL VERSION")
        print("=" * 60)
        print(f"Input: {self.a2d2_root}")
        print(f"Output: {self.output_root}")
        print()
        
        try:
            # Process all tasks
            self.process_semantic_segmentation()
            self.process_2d_detection()
            self.process_3d_detection()
            
            # Create config files
            self.create_dataset_configs()
            
            print("\n" + "=" * 60)
            print("CONVERSION COMPLETED SUCCESSFULLY!")
            print("=" * 60)
            print(f"Converted data saved to: {self.output_root}")
            print()
            print("Dataset Summary:")
            print("- 2D Object Detection: Ready for YOLOv12 training")
            print("- Semantic Segmentation: Ready for YOLOv12-seg training")
            print("- 3D Detection: Data prepared (requires specialized 3D model)")
            print()
            print("Next Steps:")
            print("1. Verify conversion: python verify_conversion.py")
            print("2. Train 2D detection: python train_2d.py")
            print("3. Train segmentation: python train_seg.py")
            
        except Exception as e:
            print(f"\n[ERROR] Conversion failed: {e}")
            import traceback
            traceback.print_exc()

def main():
    """Main function"""
    converter = FinalA2D2YOLOConverter()
    converter.run_conversion()

if __name__ == "__main__":
    main()


# Additional utility functions for validation and training

def verify_conversion():
    """Verify the conversion results"""
    data_root = Path("/home/Lambdaone/Emma/a2d2_yolo")
    
    print("=" * 50)
    print("CONVERSION VERIFICATION")
    print("=" * 50)
    
    tasks = ['2d_detection', '3d_detection', 'segmentation']
    splits = ['train', 'val', 'test']
    
    total_images = 0
    total_labels = 0
    
    for task in tasks:
        print(f"\n{task.upper()}:")
        task_images = 0
        task_labels = 0
        
        for split in splits:
            img_dir = data_root / task / split / "images"
            label_dir = data_root / task / split / "labels"
            
            if img_dir.exists() and label_dir.exists():
                img_count = len(list(img_dir.glob("*")))
                label_count = len(list(label_dir.glob("*")))
                
                print(f"  {split}: {img_count} images, {label_count} labels", end="")
                
                if img_count == label_count and img_count > 0:
                    print(" [OK]")
                elif img_count == 0:
                    print(" [EMPTY]")
                else:
                    print(" [MISMATCH]")
                
                task_images += img_count
                task_labels += label_count
            else:
                print(f"  {split}: [MISSING]")
        
        print(f"  Total: {task_images} images, {task_labels} labels")
        total_images += task_images
        total_labels += task_labels
    
    print(f"\nGRAND TOTAL: {total_images} images, {total_labels} labels")
    
    # Check config files
    print(f"\nCONFIG FILES:")
    config_files = [
        "2d_detection_config.yaml",
        "segmentation_config.yaml", 
        "3d_detection_config.yaml"
    ]
    
    for config_file in config_files:
        config_path = data_root / config_file
        if config_path.exists():
            print(f"  {config_file}: [OK]")
        else:
            print(f"  {config_file}: [MISSING]")


def create_training_scripts():
    """Create training scripts for each task"""
    
    # 2D Detection Training Script
    train_2d_script = '''#!/usr/bin/env python3
"""
Train YOLOv12 for 2D Object Detection on A2D2
"""

from ultralytics import YOLO
import torch

def train_2d_detection():
    print("Training YOLOv12 for 2D Object Detection...")
    
    # Load YOLOv12 model
    model = YOLO('yolov12n.pt')  # Start with nano for faster training
    
    # Training configuration
    results = model.train(
        data='/home/Lambdaone/Emma/a2d2_yolo/2d_detection_config.yaml',
        epochs=100,
        imgsz=640,
        batch=16,
        device='0' if torch.cuda.is_available() else 'cpu',
        workers=8,
        project='a2d2_results',
        name='2d_detection',
        save=True,
        plots=True,
        val=True,
        
        # Automotive-specific optimization
        optimizer='AdamW',
        lr0=0.01,
        lrf=0.01,
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=3,
        
        # Loss weights
        box=7.5,
        cls=0.5,
        dfl=1.5,
        
        # Augmentation for automotive data
        hsv_h=0.015,  # Slight hue variation
        hsv_s=0.7,    # Saturation variation  
        hsv_v=0.4,    # Value variation
        degrees=0.0,  # No rotation for automotive
        translate=0.1,
        scale=0.5,
        shear=0.0,
        perspective=0.0,
        flipud=0.0,   # No vertical flip
        fliplr=0.5,   # Horizontal flip OK
        mosaic=1.0,
        mixup=0.0,
        
        # Early stopping
        patience=30,
        
        # Multi-scale training
        rect=True,
        
        # Resume if exists
        resume=True
    )
    
    print(f"Training completed! Best model: {results.save_dir}/weights/best.pt")
    return results

if __name__ == "__main__":
    train_2d_detection()
'''
    
    # Segmentation Training Script
    train_seg_script = '''#!/usr/bin/env python3
"""
Train YOLOv12 for Semantic Segmentation on A2D2
"""

from ultralytics import YOLO
import torch

def train_segmentation():
    print("Training YOLOv12 for Semantic Segmentation...")
    
    # Load YOLOv12 segmentation model
    model = YOLO('yolov12n-seg.pt')
    
    # Training configuration
    results = model.train(
        data='/home/Lambdaone/Emma/a2d2_yolo/segmentation_config.yaml',
        epochs=150,
        imgsz=640,
        batch=12,  # Smaller batch for segmentation
        device='0' if torch.cuda.is_available() else 'cpu',
        workers=6,
        project='a2d2_results',
        name='segmentation',
        save=True,
        plots=True,
        val=True,
        
        # Segmentation specific parameters
        mask_ratio=4,
        overlap_mask=True,
        
        # Loss weights for segmentation
        box=7.5,
        cls=0.5,
        dfl=1.5,
        seg=2.5,  # Segmentation loss weight
        
        # Optimizer settings
        optimizer='AdamW',
        lr0=0.008,  # Lower LR for segmentation
        lrf=0.008,
        momentum=0.937,
        weight_decay=0.0005,
        warmup_epochs=5,  # More warmup
        
        # Conservative augmentation for precise masks
        hsv_h=0.01,
        hsv_s=0.5,
        hsv_v=0.3,
        degrees=0.0,
        translate=0.05,  # Less translation
        scale=0.3,
        shear=0.0,
        perspective=0.0,
        flipud=0.0,
        fliplr=0.5,
        mosaic=0.8,   # Less mosaic
        mixup=0.0,
        copy_paste=0.1,  # Copy-paste for segmentation
        
        # Early stopping
        patience=40,
        
        # Resume if exists
        resume=True
    )
    
    print(f"Training completed! Best model: {results.save_dir}/weights/best.pt")
    return results

if __name__ == "__main__":
    train_segmentation()
'''
    
    # Validation Script
    validate_script = '''#!/usr/bin/env python3
"""
Validate trained A2D2 models
"""

from ultralytics import YOLO
import torch

def validate_models():
    print("Validating A2D2 Models...")
    
    # Validate 2D Detection
    print("\\n1. Validating 2D Detection Model...")
    try:
        model_2d = YOLO('a2d2_results/2d_detection/weights/best.pt')
        results_2d = model_2d.val(
            data='/home/Lambdaone/Emma/a2d2_yolo/2d_detection_config.yaml',
            imgsz=640,
            batch=16,
            conf=0.25,
            iou=0.6,
            device='0' if torch.cuda.is_available() else 'cpu'
        )
        print(f"2D Detection mAP50: {results_2d.box.map50:.4f}")
        print(f"2D Detection mAP50-95: {results_2d.box.map:.4f}")
    except Exception as e:
        print(f"2D Detection validation failed: {e}")
    
    # Validate Segmentation
    print("\\n2. Validating Segmentation Model...")
    try:
        model_seg = YOLO('a2d2_results/segmentation/weights/best.pt')
        results_seg = model_seg.val(
            data='/home/Lambdaone/Emma/a2d2_yolo/segmentation_config.yaml',
            imgsz=640,
            batch=12,
            conf=0.25,
            iou=0.6,
            device='0' if torch.cuda.is_available() else 'cpu'
        )
        print(f"Segmentation Box mAP50: {results_seg.box.map50:.4f}")
        print(f"Segmentation Mask mAP50: {results_seg.seg.map50:.4f}")
    except Exception as e:
        print(f"Segmentation validation failed: {e}")

if __name__ == "__main__":
    validate_models()
'''
    
    # Inference Script  
    inference_script = '''#!/usr/bin/env python3
"""
Run inference on A2D2 test images
"""

from ultralytics import YOLO
import cv2
from pathlib import Path

def run_inference():
    print("Running A2D2 Inference...")
    
    # Load models
    try:
        model_2d = YOLO('a2d2_results/2d_detection/weights/best.pt')
        print("Loaded 2D detection model")
    except:
        print("2D detection model not found")
        model_2d = None
    
    try:
        model_seg = YOLO('a2d2_results/segmentation/weights/best.pt')
        print("Loaded segmentation model")
    except:
        print("Segmentation model not found")
        model_seg = None
    
    # Test images directory
    test_dir = Path('/home/Lambdaone/Emma/a2d2_yolo/2d_detection/test/images')
    output_dir = Path('inference_results')
    output_dir.mkdir(exist_ok=True)
    
    if not test_dir.exists():
        print(f"Test directory not found: {test_dir}")
        return
    
    test_images = list(test_dir.glob("*.png"))[:10]  # Test on first 10 images
    
    print(f"Running inference on {len(test_images)} test images...")
    
    for i, img_path in enumerate(test_images):
        print(f"Processing {img_path.name}...")
        
        # 2D Detection
        if model_2d:
            results_2d = model_2d(str(img_path), conf=0.25, iou=0.45)
            for r in results_2d:
                im_array = r.plot()
                cv2.imwrite(str(output_dir / f"{img_path.stem}_2d_detection.jpg"), im_array)
        
        # Segmentation
        if model_seg:
            results_seg = model_seg(str(img_path), conf=0.25, iou=0.45)
            for r in results_seg:
                im_array = r.plot()
                cv2.imwrite(str(output_dir / f"{img_path.stem}_segmentation.jpg"), im_array)
    
    print(f"Inference results saved to: {output_dir}")

if __name__ == "__main__":
    run_inference()
'''
    
    # Save scripts
    scripts_dir = Path("/home/Lambdaone/Emma/a2d2_yolov12_project/scripts")
    scripts_dir.mkdir(parents=True, exist_ok=True)
    
    scripts = {
        'train_2d.py': train_2d_script,
        'train_seg.py': train_seg_script,
        'validate_models.py': validate_script,
        'run_inference.py': inference_script,
        'verify_conversion.py': f'''#!/usr/bin/env python3
from final_a2d2_converter import verify_conversion
if __name__ == "__main__":
    verify_conversion()
'''
    }
    
    for script_name, script_content in scripts.items():
        script_path = scripts_dir / script_name
        with open(script_path, 'w') as f:
            f.write(script_content)
        script_path.chmod(0o755)  # Make executable
        print(f"[OK] Created: {script_path}")


if __name__ == "__main__":
    # Run the converter
    main()
    
    # Create additional scripts
    print("\nCreating training and validation scripts...")
    create_training_scripts()
    
    print("\nAll scripts created successfully!")
    print("\nTo use:")
    print("1. python final_a2d2_converter.py  # Convert data")
    print("2. python verify_conversion.py     # Verify conversion")
    print("3. python train_2d.py             # Train 2D detection")
    print("4. python train_seg.py            # Train segmentation")
    print("5. python validate_models.py      # Validate models")
    print("6. python run_inference.py        # Test inference")